{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e479ca4c",
   "metadata": {},
   "source": [
    "## Section 1ï¸âƒ£: Import Required Modules\n",
    "\n",
    "Import all necessary libraries and custom modules from the `src/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1ce2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 1: IMPORT MODULES\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IMPORTING MODULES FROM SRC PACKAGE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================\n",
    "# 1.1: Standard Libraries\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, auc,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"\\nâœ… Standard libraries imported\")\n",
    "\n",
    "# ============================================================\n",
    "# 1.2: Custom Modules from src/\n",
    "# ============================================================\n",
    "\n",
    "# These are the key modules we'll use throughout the notebook\n",
    "from src.models import EnhancedJointECGModel, ECGEncoder, GCNEncoder, ClassConditionedDecoder, SimpleClassifier\n",
    "from src.data_loader import get_dataloaders, load_ecg_data, get_class_weights, ECGDataset\n",
    "from src.graph_utils import build_knn_graph, compute_edge_scores\n",
    "\n",
    "print(\"âœ… Custom modules imported from src/\")\n",
    "print(\"   - src.models: Neural network architectures\")\n",
    "print(\"   - src.data_loader: Data loading utilities\")\n",
    "print(\"   - src.graph_utils: Graph construction utilities\")\n",
    "\n",
    "# ============================================================\n",
    "# 1.3: Set Up Visualization\n",
    "# ============================================================\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"\\nâœ… Visualization setup complete\")\n",
    "\n",
    "# ============================================================\n",
    "# 1.4: Set Random Seeds for Reproducibility\n",
    "# ============================================================\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(\"\\nâœ… Random seeds set for reproducibility\")\n",
    "\n",
    "# ============================================================\n",
    "# 1.5: Check GPU Availability\n",
    "# ============================================================\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nðŸš€ Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"   Current GPU memory: {torch.cuda.memory_allocated() / 1e9:.1f} GB / {torch.cuda.memory_reserved() / 1e9:.1f} GB\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba5a0c8",
   "metadata": {},
   "source": [
    "## Section 2ï¸âƒ£: Configuration and Data Loading\n",
    "\n",
    "Set up configuration, load ECG data using `src.data_loader`, and create train/validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1fe8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 2: CONFIGURATION AND DATA LOADING\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONFIGURATION AND DATA LOADING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================\n",
    "# 2.1: Configuration\n",
    "# ============================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Data paths\n",
    "    'train_path': 'examples/sample_ecgs.csv',      # â† Update if you have full dataset\n",
    "    'test_path': None,                             # â† Optional\n",
    "    \n",
    "    # Model parameters\n",
    "    'input_length': 187,\n",
    "    'latent_dim': 128,\n",
    "    'num_classes': 5,\n",
    "    \n",
    "    # Training parameters\n",
    "    'num_epochs': 50,                              # â† Reduce for demo, use 200+ for real training\n",
    "    'batch_size': 32,                              # â† Small batch for demo\n",
    "    'learning_rate': 0.001,\n",
    "    'k_neighbors': 5,\n",
    "    'device': device,\n",
    "    \n",
    "    # Loss weights\n",
    "    'lambda_class': 0.5,\n",
    "    'lambda_graph': 0.1,\n",
    "}\n",
    "\n",
    "# Class names\n",
    "CLASS_NAMES = ['Normal (N)', 'Supraventricular (S)', 'Ventricular (V)', 'Fusion (F)', 'Unknown (Q)']\n",
    "\n",
    "print(\"\\nâœ… Configuration loaded:\")\n",
    "for key, value in CONFIG.items():\n",
    "    if key != 'device':\n",
    "        print(f\"   {key:20s}: {value}\")\n",
    "\n",
    "print(f\"\\n   device              : {CONFIG['device']}\")\n",
    "\n",
    "# ============================================================\n",
    "# 2.2: Load Data Using src.data_loader Module\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nðŸ“Š Loading ECG data from {CONFIG['train_path']}...\")\n",
    "\n",
    "# Option 1: Use get_dataloaders for automatic train/val split\n",
    "train_loader, val_loader = get_dataloaders(\n",
    "    train_path=CONFIG['train_path'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    val_split=0.15,\n",
    "    normalize=True,\n",
    "    num_workers=0  # Set to 0 for Jupyter notebooks\n",
    ")\n",
    "\n",
    "# Option 2: Manual loading for more control (commented out)\n",
    "# waveforms, labels = load_ecg_data(CONFIG['train_path'], normalize=True)\n",
    "# dataset = ECGDataset(waveforms, labels)\n",
    "\n",
    "print(\"âœ… Data loaded successfully\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")\n",
    "print(f\"   Batch size: {CONFIG['batch_size']}\")\n",
    "\n",
    "# Get sample batch to verify shapes\n",
    "sample_batch_x, sample_batch_y = next(iter(train_loader))\n",
    "print(f\"\\nðŸ“ Data shapes:\")\n",
    "print(f\"   Waveforms: {sample_batch_x.shape}  (batch_size, channels, time_steps)\")\n",
    "print(f\"   Labels: {sample_batch_y.shape}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec871ae",
   "metadata": {},
   "source": [
    "## Section 3ï¸âƒ£: Initialize Model\n",
    "\n",
    "Create the Class-Conditional Graph Autoencoder, optimizer, and scheduler using modules from `src.models`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d067c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 3: INITIALIZE MODEL\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INITIALIZING MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================\n",
    "# 3.1: Create Model Instance\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nðŸ§  Creating EnhancedJointECGModel...\")\n",
    "\n",
    "model = EnhancedJointECGModel(\n",
    "    input_length=CONFIG['input_length'],\n",
    "    latent_dim=CONFIG['latent_dim'],\n",
    "    num_classes=CONFIG['num_classes']\n",
    ").to(CONFIG['device'])\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nâœ… Model created successfully\")\n",
    "print(f\"   Total parameters: {num_params:,}\")\n",
    "\n",
    "# Show model summary\n",
    "print(f\"\\nðŸ“ Model Components:\")\n",
    "print(f\"   â”œâ”€ ECGEncoder: CNN with 3 conv layers\")\n",
    "print(f\"   â”œâ”€ GCNEncoder: 2-layer Graph Convolutional Network\")\n",
    "print(f\"   â”œâ”€ ClassConditionedDecoder: Deconvolutional network\")\n",
    "print(f\"   â””â”€ SimpleClassifier: 2-layer dense network\")\n",
    "\n",
    "# ============================================================\n",
    "# 3.2: Set Up Optimizer\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nðŸ”§ Setting up optimizer...\")\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "print(f\"âœ… Optimizer configured: Adam\")\n",
    "print(f\"   Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"   Weight decay: 1e-5\")\n",
    "\n",
    "# ============================================================\n",
    "# 3.3: Learning Rate Scheduler\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nâ±ï¸  Setting up learning rate scheduler...\")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"âœ… Scheduler configured: ReduceLROnPlateau\")\n",
    "print(f\"   Reduces LR by 50% if no improvement for 5 epochs\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7982737",
   "metadata": {},
   "source": [
    "## Section 4ï¸âƒ£: Build k-NN Graphs\n",
    "\n",
    "Use `src.graph_utils.build_knn_graph()` to construct graphs from embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26867db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 4: TEST GRAPH CONSTRUCTION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING GRAPH CONSTRUCTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================\n",
    "# 4.1: Create Sample Data and Build Graph\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nðŸ”— Building k-NN graph for a sample batch...\")\n",
    "\n",
    "# Get a sample batch\n",
    "sample_x, sample_y = next(iter(train_loader))\n",
    "sample_x = sample_x.to(CONFIG['device'])\n",
    "\n",
    "# Extract features using CNN encoder\n",
    "with torch.no_grad():\n",
    "    sample_features = model.ecg_encoder(sample_x)\n",
    "\n",
    "print(f\"\\n   CNN features shape: {sample_features.shape}\")\n",
    "print(f\"   (batch_size={CONFIG['batch_size']}, feature_dim={CONFIG['latent_dim']})\")\n",
    "\n",
    "# Build k-NN graph\n",
    "edge_index = build_knn_graph(\n",
    "    sample_features,\n",
    "    k=CONFIG['k_neighbors'],\n",
    "    metric='cosine'\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Graph constructed successfully\")\n",
    "print(f\"   Edge index shape: {edge_index.shape}\")\n",
    "print(f\"   Number of edges: {edge_index.shape[1]}\")\n",
    "print(f\"   Average edges per node: {edge_index.shape[1] / CONFIG['batch_size']:.1f}\")\n",
    "print(f\"   (Expected: ~{2 * CONFIG['k_neighbors']} for bidirectional graph)\")\n",
    "\n",
    "# ============================================================\n",
    "# 4.2: Verify Graph Properties\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nðŸ” Graph Properties:\")\n",
    "\n",
    "# Check connectivity\n",
    "connected_nodes = torch.unique(edge_index)\n",
    "print(f\"   Connected nodes: {len(connected_nodes)} / {CONFIG['batch_size']}\")\n",
    "\n",
    "# Check degree distribution\n",
    "from torch_geometric.utils import degree\n",
    "deg = degree(edge_index[0], num_nodes=CONFIG['batch_size'])\n",
    "print(f\"   Node degree - Min: {deg.min().item():.1f}, Max: {deg.max().item():.1f}, Avg: {deg.mean().item():.1f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ad2c47",
   "metadata": {},
   "source": [
    "## Section 5ï¸âƒ£: Training Loop\n",
    "\n",
    "Execute the training loop with monitoring and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b49ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 5: TRAINING LOOP\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING CLASS-CONDITIONAL GAE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================\n",
    "# 5.1: Initialize Training Storage\n",
    "# ============================================================\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_recon_loss': [],\n",
    "    'train_class_loss': [],\n",
    "    'train_graph_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "# ============================================================\n",
    "# 5.2: Main Training Loop\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nðŸš€ Starting training for {CONFIG['num_epochs']} epochs...\\n\")\n",
    "\n",
    "for epoch in range(1, CONFIG['num_epochs'] + 1):\n",
    "    # ============================================================\n",
    "    # Training Phase\n",
    "    # ============================================================\n",
    "    \n",
    "    model.train()\n",
    "    epoch_losses = {'total': 0, 'recon': 0, 'class': 0, 'graph': 0}\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x = batch_x.to(CONFIG['device'])\n",
    "        batch_y = batch_y.to(CONFIG['device'])\n",
    "        \n",
    "        # Build k-NN graph\n",
    "        with torch.no_grad():\n",
    "            features = model.ecg_encoder(batch_x)\n",
    "        edge_index = build_knn_graph(features, k=CONFIG['k_neighbors'])\n",
    "        \n",
    "        # Forward pass (training mode: use TRUE labels)\n",
    "        optimizer.zero_grad()\n",
    "        z, reconstructed, logits = model(batch_x, edge_index, batch_y)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, loss_dict = model.compute_loss(\n",
    "            batch_x, reconstructed, logits, batch_y, edge_index, z\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate losses\n",
    "        epoch_losses['total'] += loss.item()\n",
    "        epoch_losses['recon'] += loss_dict['recon']\n",
    "        epoch_losses['class'] += loss_dict['class']\n",
    "        epoch_losses['graph'] += loss_dict['graph']\n",
    "        num_batches += 1\n",
    "    \n",
    "    # Average losses\n",
    "    for key in epoch_losses:\n",
    "        epoch_losses[key] /= num_batches\n",
    "    \n",
    "    history['train_loss'].append(epoch_losses['total'])\n",
    "    history['train_recon_loss'].append(epoch_losses['recon'])\n",
    "    history['train_class_loss'].append(epoch_losses['class'])\n",
    "    history['train_graph_loss'].append(epoch_losses['graph'])\n",
    "    \n",
    "    # ============================================================\n",
    "    # Validation Phase\n",
    "    # ============================================================\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for val_x, val_y in val_loader:\n",
    "            val_x = val_x.to(CONFIG['device'])\n",
    "            val_y = val_y.to(CONFIG['device'])\n",
    "            \n",
    "            # Build graph\n",
    "            val_features = model.ecg_encoder(val_x)\n",
    "            val_edge_index = build_knn_graph(val_features, k=CONFIG['k_neighbors'])\n",
    "            \n",
    "            # Inference mode: use PREDICTED classes\n",
    "            z_val, recon_val, logits_val, _ = model.forward_inference(val_x, val_edge_index)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss_val, _ = model.compute_loss(\n",
    "                val_x, recon_val, logits_val, val_y, val_edge_index, z_val\n",
    "            )\n",
    "            val_loss += loss_val.item()\n",
    "            \n",
    "            # Accuracy\n",
    "            pred_classes = logits_val.argmax(dim=1)\n",
    "            correct += (pred_classes == val_y).sum().item()\n",
    "            total += val_y.size(0)\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = correct / total\n",
    "    \n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), 'best_gae_model.pth')\n",
    "        save_indicator = \" ðŸ’¾ SAVED\"\n",
    "    else:\n",
    "        save_indicator = \"\"\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch:3d}/{CONFIG['num_epochs']} | \"\n",
    "              f\"Recon: {epoch_losses['recon']:.4f} | \"\n",
    "              f\"Class: {epoch_losses['class']:.4f} | \"\n",
    "              f\"Graph: {epoch_losses['graph']:.4f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "              f\"Val Acc: {val_acc:.4f}{save_indicator}\")\n",
    "\n",
    "# ============================================================\n",
    "# 5.3: Training Summary\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"âœ… TRAINING COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nðŸ† Best Results:\")\n",
    "print(f\"   Epoch: {best_epoch}/{CONFIG['num_epochs']}\")\n",
    "print(f\"   Validation Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
    "print(f\"   Model saved: best_gae_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72869def",
   "metadata": {},
   "source": [
    "## Section 6ï¸âƒ£: Evaluate and Visualize Results\n",
    "\n",
    "Extract latent representations, visualize with t-SNE, and compute comprehensive metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c360ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 6: EVALUATE AND VISUALIZE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION AND VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================\n",
    "# 6.1: Load Best Model and Extract Predictions\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nðŸ“Š Loading best model and extracting predictions...\")\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_gae_model.pth', map_location=CONFIG['device']))\n",
    "model.eval()\n",
    "\n",
    "# Collect all predictions and representations\n",
    "all_latents = []\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "all_reconstructed = []\n",
    "all_original = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for val_x, val_y in val_loader:\n",
    "        val_x = val_x.to(CONFIG['device'])\n",
    "        \n",
    "        # Build graph\n",
    "        val_features = model.ecg_encoder(val_x)\n",
    "        val_edge_index = build_knn_graph(val_features, k=CONFIG['k_neighbors'])\n",
    "        \n",
    "        # Inference\n",
    "        z_val, recon_val, logits_val, pred_classes = model.forward_inference(val_x, val_edge_index)\n",
    "        \n",
    "        # Collect data\n",
    "        all_latents.append(z_val.cpu().numpy())\n",
    "        all_predictions.append(pred_classes.cpu().numpy())\n",
    "        all_true_labels.append(val_y.cpu().numpy())\n",
    "        all_reconstructed.append(recon_val.cpu().numpy())\n",
    "        all_original.append(val_x.cpu().numpy())\n",
    "\n",
    "# Concatenate\n",
    "latent_np = np.concatenate(all_latents, axis=0)\n",
    "predictions_np = np.concatenate(all_predictions, axis=0)\n",
    "true_labels_np = np.concatenate(all_true_labels, axis=0)\n",
    "reconstructed_np = np.concatenate(all_reconstructed, axis=0)\n",
    "original_np = np.concatenate(all_original, axis=0)\n",
    "\n",
    "print(f\"âœ… Predictions collected\")\n",
    "print(f\"   Latent representations: {latent_np.shape}\")\n",
    "print(f\"   Total validation samples: {len(predictions_np):,}\")\n",
    "\n",
    "# ============================================================\n",
    "# 6.2: Plot Training History\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nðŸ“ˆ Plotting training history...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['train_loss'], label='Total Train Loss', linewidth=2.5, color='steelblue')\n",
    "axes[0].plot(history['val_loss'], label='Total Val Loss', linewidth=2.5, color='darkorange')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Accuracy curves\n",
    "axes[1].plot(history['val_acc'], label='Validation Accuracy', linewidth=2.5, color='seagreen')\n",
    "axes[1].axhline(best_val_acc, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Best: {best_val_acc:.4f} (Epoch {best_epoch})')\n",
    "axes[1].fill_between(range(len(history['val_acc'])), history['val_acc'], alpha=0.3, color='seagreen')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Classification Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0, 1.05])\n",
    "\n",
    "plt.suptitle(f'Training Progress (Best Val Acc: {best_val_acc:.2%})', \n",
    "             fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Training history saved as: training_history.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dbfbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6.3: Compute Classification Metrics\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLASSIFICATION METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Overall accuracy\n",
    "overall_accuracy = accuracy_score(true_labels_np, predictions_np)\n",
    "macro_f1 = f1_score(true_labels_np, predictions_np, average='macro')\n",
    "weighted_f1 = f1_score(true_labels_np, predictions_np, average='weighted')\n",
    "\n",
    "print(f\"\\nâœ… Overall Performance:\")\n",
    "print(f\"   Accuracy:      {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)\")\n",
    "print(f\"   Macro F1:      {macro_f1:.4f}\")\n",
    "print(f\"   Weighted F1:   {weighted_f1:.4f}\")\n",
    "\n",
    "# Per-class metrics\n",
    "print(f\"\\nâœ… Per-Class Performance:\")\n",
    "print(classification_report(true_labels_np, predictions_np, \n",
    "                          target_names=CLASS_NAMES, digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels_np, predictions_np)\n",
    "\n",
    "# ============================================================\n",
    "# 6.4: Visualize with t-SNE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nðŸ” Computing t-SNE visualization...\")\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=SEED, perplexity=30, n_iter=1000)\n",
    "latent_2d = tsne.fit_transform(latent_np)\n",
    "\n",
    "print(\"âœ… t-SNE computed\")\n",
    "\n",
    "# Plot t-SNE with predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "colors = ['#2ecc71', '#e74c3c', '#3498db', '#f39c12', '#9b59b6']\n",
    "\n",
    "# True classes\n",
    "ax = axes[0]\n",
    "for class_id in range(5):\n",
    "    mask = true_labels_np == class_id\n",
    "    ax.scatter(latent_2d[mask, 0], latent_2d[mask, 1],\n",
    "              s=50, alpha=0.7, c=colors[class_id],\n",
    "              label=CLASS_NAMES[class_id],\n",
    "              edgecolors='black', linewidth=0.5)\n",
    "\n",
    "ax.set_title('t-SNE: Ground Truth Classes', fontsize=15, fontweight='bold')\n",
    "ax.set_xlabel('t-SNE Component 1', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('t-SNE Component 2', fontsize=12, fontweight='bold')\n",
    "ax.legend(loc='best', framealpha=0.95, fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Predicted classes\n",
    "ax = axes[1]\n",
    "for class_id in range(5):\n",
    "    mask = predictions_np == class_id\n",
    "    ax.scatter(latent_2d[mask, 0], latent_2d[mask, 1],\n",
    "              s=50, alpha=0.7, c=colors[class_id],\n",
    "              label=CLASS_NAMES[class_id],\n",
    "              edgecolors='black', linewidth=0.5)\n",
    "\n",
    "ax.set_title(f't-SNE: Predicted Classes (Acc: {overall_accuracy:.1%})', \n",
    "            fontsize=15, fontweight='bold')\n",
    "ax.set_xlabel('t-SNE Component 1', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('t-SNE Component 2', fontsize=12, fontweight='bold')\n",
    "ax.legend(loc='best', framealpha=0.95, fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Latent Space Visualization: True vs Predicted', \n",
    "            fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('tsne_visualization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… t-SNE visualization saved as: tsne_visualization.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966d6294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6.5: Comprehensive Performance Report\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPREHENSIVE PERFORMANCE REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Confusion Matrix\n",
    "ax = axes[0, 0]\n",
    "conf_matrix_norm = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(conf_matrix_norm, annot=True, fmt='.3f', cmap='RdYlGn',\n",
    "           xticklabels=[name.split(' (')[0] for name in CLASS_NAMES],\n",
    "           yticklabels=[name.split(' (')[0] for name in CLASS_NAMES],\n",
    "           ax=ax, vmin=0, vmax=1, cbar_kws={'label': 'Proportion'})\n",
    "ax.set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Plot 2: Per-Class Accuracy\n",
    "ax = axes[0, 1]\n",
    "class_accuracies = []\n",
    "for class_id in range(5):\n",
    "    mask = true_labels_np == class_id\n",
    "    if mask.sum() > 0:\n",
    "        acc = (predictions_np[mask] == class_id).mean()\n",
    "        class_accuracies.append(acc)\n",
    "    else:\n",
    "        class_accuracies.append(0)\n",
    "\n",
    "bars = ax.bar(range(5), class_accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "ax.axhline(overall_accuracy, color='red', linestyle='--', linewidth=2, label=f'Overall: {overall_accuracy:.3f}')\n",
    "ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Per-Class Accuracy', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels([name.split(' (')[0] for name in CLASS_NAMES], rotation=45, ha='right')\n",
    "ax.set_ylim([0, 1.1])\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: Loss Component Breakdown\n",
    "ax = axes[1, 0]\n",
    "x = np.arange(len(history['train_loss']))\n",
    "ax.plot(x, history['train_recon_loss'], label='Reconstruction', linewidth=2, color='#3498db')\n",
    "ax.plot(x, history['train_class_loss'], label='Classification', linewidth=2, color='#e74c3c')\n",
    "ax.plot(x, history['train_graph_loss'], label='Graph', linewidth=2, color='#2ecc71')\n",
    "ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Loss Components Over Time', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Summary Statistics\n",
    "ax = axes[1, 1]\n",
    "ax.axis('off')\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "MODEL PERFORMANCE SUMMARY\n",
    "{'='*40}\n",
    "\n",
    "Overall Metrics:\n",
    "  â€¢ Accuracy:           {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)\n",
    "  â€¢ Macro F1-Score:     {macro_f1:.4f}\n",
    "  â€¢ Weighted F1-Score:  {weighted_f1:.4f}\n",
    "\n",
    "Training Details:\n",
    "  â€¢ Best Epoch:         {best_epoch}/{CONFIG['num_epochs']}\n",
    "  â€¢ Best Val Accuracy:  {best_val_acc:.4f}\n",
    "  â€¢ Total Samples:      {len(true_labels_np):,}\n",
    "  â€¢ Correct Preds:      {(predictions_np == true_labels_np).sum():,}\n",
    "  â€¢ Incorrect Preds:    {(predictions_np != true_labels_np).sum():,}\n",
    "\n",
    "Model Architecture:\n",
    "  â€¢ Total Parameters:   {sum(p.numel() for p in model.parameters()):,}\n",
    "  â€¢ Latent Dimension:   {CONFIG['latent_dim']}\n",
    "  â€¢ k-NN Neighbors:     {CONFIG['k_neighbors']}\n",
    "  â€¢ Device:             {CONFIG['device']}\n",
    "\"\"\"\n",
    "\n",
    "ax.text(0.1, 0.95, summary_text, transform=ax.transAxes,\n",
    "       fontsize=11, verticalalignment='top', family='monospace',\n",
    "       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.suptitle('Comprehensive Model Evaluation', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_evaluation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Evaluation report saved as: model_evaluation.png\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddd87cc",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ“š Next Steps\n",
    "\n",
    "**To use this for your own data:**\n",
    "\n",
    "1. Update `CONFIG['train_path']` with your CSV file path\n",
    "2. Adjust class names if needed\n",
    "3. Run all cells again\n",
    "\n",
    "**To deploy in production:**\n",
    "\n",
    "```python\n",
    "# Use src modules in FastAPI\n",
    "from src.models import EnhancedJointECGModel\n",
    "from src.inference import ECGPredictor\n",
    "\n",
    "# Or use via CLI\n",
    "python src/train.py --data data.csv --epochs 200\n",
    "python src/inference.py --model best_model.pth --input test.csv\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
